# パフォーマンスボトルネック最適化

## はじめに

SIP負荷試験ツールのパフォーマンスボトルネックを特定・解消するための最適化を実施しました。Go言語版と比較してパフォーマンスが劣っていた問題を改善するため、各レイヤーに対して段階的に最適化を行いました。

## 最適化領域

| 領域 | 問題 | 対策 |
|------|------|------|
| SIPパーサー | 毎回String/Vec<u8>をヒープ確保 | ゼロコピーパース、頻出ヘッダのインデックスキャッシュ |
| SIPフォーマッター | format!マクロ多用、バッファ未確保 | 事前確保バッファへの直接書き込み、`format_into` API追加 |
| トランスポート | 単一ソケット送信、バッファ毎回確保 | ラウンドロビン送信、スタック上バッファ使用 |
| 受信ループ | 100msポーリング | `tokio::select!`による非同期待機、複数ソケット並行受信 |
| プロキシ | 全メッセージ再シリアライズ | バッファ再利用、高速RNG（`rand::thread_rng`） |
| 統計収集 | `Mutex<Vec<Duration>>`によるロック競合 | シャーディングバッファ（CPUコア数ベース） |
| ダイアログ管理 | `all_call_ids()`で全エントリクローン | イテレータベース処理、`SmallRng`導入 |
| メッセージ構築 | format!マクロ多用 | 事前確保バッファへの直接書き込み |
| 非同期タスク | 個別tokio::spawn | BYEバッチ送信、動的CPS間隔調整 |

## 設計方針

- 後方互換性の維持: 既存のAPIシグネチャと動作を維持し、全既存テストが通過すること
- 段階的最適化: 各コンポーネントを独立して最適化可能にし、ベンチマークで効果を測定
- ゼロコスト抽象化の活用: Rustの所有権システムとライフタイムを活用
- TDDアプローチ: テストファーストで各最適化を実装

## 主要な変更点

### Headers構造体の拡張

頻出ヘッダ（Via, From, To, Call-ID, CSeq, Content-Length）へのインデックスキャッシュを追加し、`get()`で定数時間アクセスを実現。`SmallVec`を使用してViaヘッダのインデックスをスタック上に保持。

### トランスポート層

`UdpTransport`に`AtomicUsize`カウンタを追加し、`send_to`でラウンドロビンソケット選択を実装。受信バッファをスタック上の`[u8; 65535]`に変更。

### 統計収集

`Mutex<Vec<Duration>>`をCPUコア数ベースのシャーディングバッファに変更。パーセンタイル計算時に全シャードをマージ。

### 受信ループ

100msタイムアウトポーリングを`tokio::select!`に変更。複数ソケットの並行受信を実装。

### 非同期タスク管理

BYE送信を`tokio::time::interval`によるバッチ処理に変更。コール送信間隔をCPSベースの動的計算に変更。

## 正当性プロパティ

以下のプロパティをproptestで検証済み:

1. Format→Parseラウンドトリップ
2. ヘッダ検索のインデックスキャッシュ整合性
3. フォーマッター出力のバイト列一致
4. ラウンドロビン送信の均等性
5. プロキシリクエスト転送のVia/Record-Route正確性
6. Branch値の一意性
7. プロキシレスポンス転送のVia除去正確性
8. パーセンタイル計算の等価性
9. タイムアウト検出の正確性
10. 構築メッセージのパース可能性
11. ベンチマーク結果のJSONラウンドトリップ

## パフォーマンスプロファイリングと最適化（第2フェーズ）

### はじめに

第1フェーズの最適化（ゼロコピーパース、ラウンドロビン送信、シャーディングバッファ等）に続き、samplyプロファイラによるボトルネック特定と段階的な最適化を実施しました。主にOrchestratorの送信ループとドレインフェーズに焦点を当て、max_stable_cpsの大幅な向上を目指しました。

### 最適化領域

| 領域 | 問題 | 対策 |
|------|------|------|
| ドレインフェーズ | 各ステップ終了時に最大20秒の固定待機が発生 | 早期終了判定（active_dialogs=0で即座に終了）、タイムアウト時の残存ダイアログ強制クリーンアップ |
| コール送信 | send_invite/send_registerの逐次awaitでスループット制限 | tokio::JoinSetによる並行発行 |
| 送信間隔計算 | calculate_send_intervalの粒度が粗くバースト送信に非対応 | 戻り値を`(Duration, u64)`タプルに変更し、高CPS時のバッチ送信に対応 |
| ベンチマーク設定 | max_dialogs=500で高CPS時にMaxDialogsReached頻発 | CPSとcall_durationに基づく動的算出（詳細は [SIP負荷試験ツール](sip-load-tester.md) の max_dialogs 自動計算セクションを参照） |
| StatsCollector | 並行送信環境下での統計整合性が未検証 | 並行安全性の検証と保証 |
| プロファイリング環境 | プロファイリング手順が未整備 | デバッグシンボル付きリリースビルドとsamplyプロファイリングスクリプト整備 |

### 設計方針

- プロファイリング駆動: samplyプロファイラでボトルネックを特定してから最適化を実施
- 段階的改善: 各最適化を独立して適用・測定し、効果を定量的に検証
- 後方互換性の維持: 既存のAPI・テストを壊さない範囲で最適化
- TDDアプローチ: テストファーストで各最適化を実装

### 主要な変更点

#### プロファイリング環境整備

`Cargo.toml`の`[profile.release]`に`debug = true`を追加し、リリースビルドにデバッグシンボルを含めるようにしました。`scripts/bench_profile.sh`を新規作成し、samplyプロファイラでラップしたベンチマーク実行をワンコマンドで行えるようにしました。

#### ドレインフェーズ最適化

`run_step`のドレインフェーズを改善しました。active_dialogsが0になった時点で即座にドレインを終了し、タイムアウト（`call_duration + shutdown_timeout`秒）時は残存ダイアログを強制クリーンアップします。`DialogManager::force_remove_all()`と`Orchestrator::force_cleanup_remaining_dialogs()`を新規追加し、クリーンアップされたダイアログはfailed_callsに計上されます。ドレインフェーズ中もshutdown_flagを確認し、シャットダウン要求があれば即座に終了します。

#### コール送信の並行化

送信ループ内のsend_invite/send_registerの逐次awaitを`tokio::JoinSet`による並行発行に変更しました。各コールは独立してエラーハンドリングされ、1つのコール失敗が他のコールに影響を与えません。OSスレッドを追加消費せず、tokioの非同期タスクのみで実現しています。

#### 送信間隔計算の精度向上

`calculate_send_interval`関数の戻り値を`Duration`から`(Duration, u64)`タプルに変更しました。高CPS（100以上）では10msインターバルでバッチ送信を行い、目標CPSに対して±5%以内の精度を実現します。CPS=0以下の場合はパニックせず安全なデフォルト値を返し、送信間隔は常に1ms〜100msの範囲内に収まります。

#### ベンチマーク設定の最適化

`bench_profile.sh`のベンチマーク設定パラメータを最適化しました。`max_dialogs` は自動計算に移行し、`call_duration` と `shutdown_timeout` を短縮してベンチマーク高速化を実現しています。

#### StatsCollectorの並行安全性保証

並行送信環境下でのStatsCollectorの統計整合性を検証しました。複数の非同期タスクが同時にrecord_call/record_failureを呼び出しても全ての呼び出しが漏れなく記録され、`total_calls = successful_calls + failed_calls`の不変条件が維持されることを確認しています。

### 正当性プロパティ

以下のプロパティをproptestで検証済み:

1. 強制クリーンアップの完全性: force_cleanup後にactive_dialogs=0、failed_callsにN加算
2. 並行記録の完全性: N×Mの並行record_call呼び出しが全て記録される
3. StatsCollectorのtotal_calls不変条件: total_calls = successful_calls + failed_calls
4. コール失敗の独立性: k個のコール失敗が残りN-k個のコールに影響しない
5. max_dialogs制限の遵守: M個を超えるダイアログ作成がMaxDialogsReachedエラーを返す
6. 送信間隔の精度: 実効CPSが目標CPSの±5%以内
7. 高CPSでのバッチ送信: CPS≥100でbatch_size>1
8. 送信間隔の安全性: 任意のf64入力でパニックせず、intervalが1ms〜100ms範囲内

## recv_loop タスク生成オーバーヘッドの排除（第3フェーズ）

### はじめに

第2フェーズの最適化後、samplyプロファイラによる追加分析で `recv_loop` 内のメッセージディスパッチにおける `tokio::spawn` が新たなボトルネックとして特定されました。CPU時間の約52%がタスク生成・解放のオーバーヘッドに費やされていました。

- 47.0%: `__rust_dealloc`/`free()`（タスクのメモリ解放）
- 5.3%: `Harness::complete`（tokioタスクのクリーンアップ）

### 最適化領域

| 領域 | 問題 | 対策 |
|------|------|------|
| recv_loop メッセージディスパッチ | 受信メッセージごとに `tokio::spawn` で新規タスク生成 | インライン `.await` に置き換え |
| Arc クローン | `tokio::spawn` 用に毎回 `Arc<SipProxy>` をクローン | クローン不要（ループ外の参照を直接使用） |

### 設計方針

- 最小限の変更: `recv_loop` 内のメッセージディスパッチブロック1箇所のみを変更
- 関数シグネチャ不変: `recv_loop` の引数・戻り値型は一切変更しない
- 並行性の維持: `main` 関数が `bind_count × recv_task_count` 個の `recv_loop` タスクを起動する構造は変更なし
- TDDアプローチ: テストファーストで実装

### 変更内容

`recv_loop` 関数内のメッセージディスパッチブロックから `tokio::spawn` ラッパーと `Arc<SipProxy>` クローンを削除し、`handle_request` / `handle_response` をインラインで `.await` する方式に変更しました。

変更前は各 `recv_loop` タスク内でさらに `tokio::spawn` による二重並行化が行われていましたが、`handle_request` / `handle_response` はSIPメッセージの加工とUDP送信を行う比較的高速な処理であり、タスク生成のオーバーヘッドが並行化の恩恵を上回っていました。

### 正当性プロパティ

以下のプロパティをproptestで検証済み:

1. エラー耐性: `handle_request` / `handle_response` がエラーを返した場合でも `recv_loop` が停止せず次のメッセージ受信を継続する（100イテレーション）

### ベンチマーク結果

| 指標 | 変更前 | 変更後 |
|------|--------|--------|
| max_stable_cps | 1,680 | 1,680 |

ベースラインCPS（1,680）を維持しており、性能リグレッションなしを確認しました。
